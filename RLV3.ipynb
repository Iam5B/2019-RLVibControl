{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "class platform:\n",
    "    def __init__(self, steps = 1, controlarr = np.array(range(-1100,1200,100)), maxX = 0.015, minX = 0.001):\n",
    "        # 平台模拟对象的构造函数\n",
    "        # 支持设置输入的步数 step\n",
    "        # 支持传入控制力差分量序列 controlarr\n",
    "        # 支持传入两个阈值项maxX minX\n",
    "        \n",
    "        self.GoodReward = 50\n",
    "        self.BadReward = -20\n",
    "        self.NormalReward = 1\n",
    "        # 一些打分量，可以自行修改\n",
    "        \n",
    "        self.steps = steps;\n",
    "        # 读取的状态步数，1表示只读取当前时刻的状态向量，2表示当前加上前一刻的状态\n",
    "        \n",
    "        self.fg = 2\n",
    "        # 与matlab版程序的fg含义相同，但是要注意是从0开始的\n",
    "        \n",
    "        self.h = 10\n",
    "        # 时延项\n",
    "        \n",
    "        self.N = 5000\n",
    "        # 时间步数\n",
    "        \n",
    "        self.maxX = maxX\n",
    "        self.minX = minX\n",
    "        # 截留一些阈值\n",
    "        \n",
    "        self.controlarr = controlarr\n",
    "        # 保存一下离散的控制力差分量\n",
    "        \n",
    "        self.n_actions = controlarr.shape[0]\n",
    "        # 记录一下控制力差分量的可选个数\n",
    "        \n",
    "        self.n_features = steps * 4 + 1\n",
    "        # 网络的输入维度 4 * 步数 + 1，那个多出的1是归一化后的控制力，位于输入的最后一维，即[-50000,50000] => [-1, 1]\n",
    "        \n",
    "        raw_data = sio.loadmat('waveforce081228.mat')\n",
    "        # 只需要是那个包含了f矩阵的.mat文件即可，f矩阵表示了若干个时刻下的随机波浪力\n",
    "        \n",
    "        self.matA = np.array([0.9998, 0.0000, 0.0100, 0.0000,0.0003, 0.9997, 0.0000 ,0.0100,-0.0486, 0.0003, 0.9980, 0.0000,0.0541, -0.0542, 0.0046, 0.9954]).reshape([4, 4])\n",
    "        # 对应于原matlab代码中的A B D B1 Bc Dc Ac矩阵\n",
    "        # 用matlab在给定平台和AMD参数后运行一遍simulation.m后即可得到各个矩阵的值\n",
    "        # 如果matlab里面的平台参数没变此处可以保留，已经严格检验过正确性\n",
    "        \n",
    "        self.matB = np.array([0.0004,-0.0811,-0.0043,0.8580]).reshape([4, 1])\n",
    "        self.matB = self.matB * 1e-6\n",
    "\n",
    "        self.matD = np.array([0.0021,0.0000,0.4213,0.0010]).reshape([4, 1])\n",
    "        self.matD = self.matD * 1e-8\n",
    "\n",
    "        self.matB1 = np.array([-0.0000,0.0042,-0.0042,0.8416]).reshape([4, 1])\n",
    "        self.matB1 = self.matB1 * 1e-6\n",
    "\n",
    "        self.matBc = np.array([0,0,-0.0042,0.8435]).reshape([4, 1])\n",
    "        self.matBc = self.matBc * 1e-4\n",
    "\n",
    "        self.matDc = np.array([0,0,0.4217,0]).reshape([4, 1])\n",
    "        self.matDc = self.matDc * 1e-6\n",
    "\n",
    "        self.matAc = np.array([0,0,1.0000,0,\n",
    "                            0,0,0,1.0000,\n",
    "                            -4.8671,0.0271,-0.1782,0.0022,\n",
    "                            5.4289,-5.4289,0.4343,-0.4343]).reshape([4, 4])\n",
    "\n",
    "        \n",
    "        self.matx = np.zeros([4, self.N, 4])\n",
    "        self.mata = np.zeros([1, self.N, 4])\n",
    "        self.matz = np.zeros([4, self.N, 4])\n",
    "        self.matu = np.zeros([1, self.N, 4])\n",
    "        # 对应于matlab中的x a z u四部分矩阵\n",
    "        # 为了方便后续代码直接移植，所以可能多开辟了一些空间，但是方便后面的下标对应\n",
    "    \n",
    "        self.matf = np.array(raw_data['f'])\n",
    "        # 存储随机波浪力\n",
    "\n",
    "    def reset(self):\n",
    "        # 这是进行一个episode前的准备步骤\n",
    "        # 将记录平台历史状态的数组清空\n",
    "        # 将每个step中做出的选择的记录清空\n",
    "\n",
    "        self.matx = np.zeros([4, self.N, 4])\n",
    "        self.mata = np.zeros([1, self.N, 4])\n",
    "        self.matz = np.zeros([4, self.N, 4])\n",
    "        self.matu = np.zeros([1, self.N, 4])\n",
    "        # 将历史状态数组置0\n",
    "        \n",
    "        self.k = self.h\n",
    "        # 这个k是指向当前时刻的游标，由于时延的关系就直接设置为了h\n",
    "        \n",
    "        self.force = 0\n",
    "        # 这个是当前的控制力\n",
    "    \n",
    "        self.forcev = 0\n",
    "        # 这个是当前的控制力变化速度\n",
    "        # 当用到二阶加速度时可能会用到这个量\n",
    "        \n",
    "        returnvalue = self.matz[:, self.k-self.steps+1:self.k+1, self.fg].reshape([n_features-1])\n",
    "        returnvalue = np.append(returnvalue, self.force/50000)\n",
    "        # 这里是初始化了第一个时刻接收到的状态序列，并拼接上了归一化的控制力\n",
    "        # 注意[n_features - 1]是因为n_features里面包含了那个控制力的维度，所以要减1才能对应的上steps步 * 4个维度\n",
    "        return returnvalue\n",
    "    \n",
    "    def showlast(self):\n",
    "        # 打印最近一个episode中平台的状态表现\n",
    "        # 本实验中我无法分辨x 和 z到底用哪个，因此做个说明\n",
    "        # 所有展示用的都是x矩阵中的状态量\n",
    "        # 所有涉及计算过程的都是用的z矩阵中的状态量\n",
    "        # 有问题请自行修改\n",
    "        \n",
    "        plt.plot(range(0,self.k),self.matx[0,range(0,self.k),self.fg])\n",
    "        plt.show()\n",
    "        # 打印偏移\n",
    "        # plot功能在jupyter下启动需要加 %matplotlib inline，已经加在前面了\n",
    "        \n",
    "        \n",
    "        plt.plot(range(0,self.k),self.matu[0,range(0,self.k),self.fg])\n",
    "        plt.show()\n",
    "        # 打印控制力输出\n",
    "        \n",
    "    def save_status(self, str):\n",
    "        # 指定名称保存某个episode中的平台状态历史记录，也即模型表现，可以保存为.mat\n",
    "        \n",
    "        sio.savemat(str,{'x':self.matx,'a':self.mata,'z':self.matz,'u':self.matu})\n",
    "        # 保存了x a z u四个矩阵，matlab下可以正常读取\n",
    "        \n",
    "    def maxabsx(self):\n",
    "        # 用于计算平台在上一个episode中的最大偏移值\n",
    "        \n",
    "        return np.max(np.abs(self.matx[0,:,]))\n",
    "        # 注意这里好像没有指定fg,但是应该不影响效果\n",
    "    \n",
    "    def stdx(self):\n",
    "        # 同上 用于计算平台在上一个episode中的均方差\n",
    "        \n",
    "        xs = self.matx[0,:,self.fg]\n",
    "        # 注意这里需要指定第三个维度为fg\n",
    "        \n",
    "        xs = xs - np.mean(xs)\n",
    "        xs = np.multiply(xs, xs)\n",
    "        xs = np.sum(xs)\n",
    "        xs = np.sqrt(xs / self.N)\n",
    "        return xs\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        # 接受控制\n",
    "        # 返回状态和即时奖励\n",
    "        \n",
    "        deltav = self.controlarr[action]\n",
    "        # 这里是用action游标指定要采用第几个差分量\n",
    "        \n",
    "        # self.forcev = self.forcev + deltav\n",
    "        # self.force = self.force + self.forcev\n",
    "        # 用二阶差分量方法时采用这里的代码\n",
    "        \n",
    "        self.force = self.force + deltav\n",
    "        # 使用一阶差分量来进行控制力的变化\n",
    "        \n",
    "        if self.force > 50000:\n",
    "            self.force = 50000\n",
    "        elif self.force < -50000:\n",
    "            self.force = -50000\n",
    "        # 控制力的上下截断\n",
    "\n",
    "        self.matu[:, self.k, self.fg] = self.force\n",
    "        # 将控制力保存在第k步\n",
    "        # 为了不混淆，我们认为现在是在第k步\n",
    "        \n",
    "        self.matz[:, self.k + 1, self.fg] = self.matA @ self.matz[:, self.k, self.fg] + self.matB @ self.matu[:, self.k, self.fg] + self.matD @ self.matf[:, self.k]\n",
    "        # 根据第k步的控制力计算出第 k + 1 步时的状态(供计算用)\n",
    "        \n",
    "        sumdelayu = np.zeros([1, 1])# strange\n",
    "        for ik in range(self.k - self.h, self.k):\n",
    "            Ak_ik_1 = np.eye(list(self.matA.shape)[0])\n",
    "            A_h = np.eye(list(self.matA.shape)[0])\n",
    "            for i in range(0, self.k-ik-1):\n",
    "                Ak_ik_1 = Ak_ik_1 @ self.matA\n",
    "            for i in range(0, self.h):\n",
    "                A_h = A_h @ np.linalg.inv(self.matA)\n",
    "            sumdelayu = sumdelayu + Ak_ik_1 @ A_h @ self.matB1 @ self.matu[:, ik, self.fg]\n",
    "        self.matx[:, self.k, self.fg] = self.matz[:, self.k, self.fg] - sumdelayu\n",
    "        # 含时滞项的计算(算出来的状态供展示用)\n",
    "        # 虽然能区分z系统和x系统，但是依然不能明白为什么要分开为两个系统进行计算\n",
    "        # 实际部署时明明只会有一个外部环境系统\n",
    "        # 另外前面一段中matz的时间下标与matx的时间下标不一致，不知道有没有弄错\n",
    "        # 再另外，如果担心该数值计算过程出错，可以自己比照原MATLAB代码\n",
    "        \n",
    "        self.mata[:, self.k, self.fg] = np.array([[0, 0, 1, 0]]) @ (self.matAc @ self.matx[:, self.k, self.fg] + self.matBc @ self.matu[:, self.k - self.h, self.fg] + self.matDc @ self.matf[:, self.k])\n",
    "        # 计算加速度\n",
    "                                                                                                          \n",
    "        self.k = self.k + 1\n",
    "        # 游标往前移动一位，这样返回的就是下一步的状态了\n",
    "        \n",
    "        returnvalue = self.matz[:, self.k-self.steps+1:self.k+1, self.fg].reshape([n_features-1])\n",
    "        returnvalue = np.append(returnvalue, self.force/50000)\n",
    "        # 截取下一步状态量（包含共steps步的状态）并拼上一个归一化控制力值\n",
    "        \n",
    "        \n",
    "        # 以下是奖励设置与返回部分\n",
    "        if self.k == self.N - 1:\n",
    "            # 过完了N步，一个episode结束\n",
    "            # 返回值中的True用来驱动学习体进行反向传播调参\n",
    "            return returnvalue,  self.NormalReward, True,           \"zzz\"          \n",
    "            #      返回状态序列，奖励值，           是否结束的标记，备用描述字符串\n",
    "            \n",
    "        if abs(self.matz[0, self.k, self.fg]) > self.maxX and abs(self.matz[0, self.k - 2, self.fg]) <= self.maxX:\n",
    "            # 表示第k + 1步的偏移，在第k步的控制力作用下开始超出了阈值（注意是开始超出，所以后面又加了一个条件）\n",
    "            return returnvalue, self.BadReward, False, \"zzz\"\n",
    "        \n",
    "        if abs(self.matz[0, self.k, self.fg]) < self.minX and abs(self.matz[0, self.k - 2, self.fg]) >= self.minX:\n",
    "            # 表示.......................................开始小于阈值\n",
    "            return returnvalue, self.GoodReward, False, \"zzz\" \n",
    "        \n",
    "        if abs(self.matz[0, self.k, self.fg] - self.matz[0, self.k - 1, self.fg]) <= 2e-6:\n",
    "            # 表示.........的瞬时Δx小于某阈值(也即速度，但是很奇怪的是matz[1,:,:]的计算有时会出错导致没法用)\n",
    "            return returnvalue, self.GoodReward, False, \"zzz\" \n",
    "        \n",
    "        if abs(self.matz[0, self.k, self.fg]) > self.maxX:\n",
    "            # 表示.......................................持续高于阈值\n",
    "            return returnvalue, self.BadReward/10, False, \"zzz\" \n",
    "        \n",
    "        return returnvalue, self.NormalReward, False, \"zzz\" \n",
    "        # 表示现在正处于一般振动情况\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part of code is the reinforcement learning brain, which is a brain of the agent.\n",
    "All decisions are made in here.\n",
    "Policy Gradient, Reinforcement Learning.\n",
    "Using:\n",
    "Tensorflow: 1.4\n",
    "gym: 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# reproducible\n",
    "# 这个过程是为了让结果可复现\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "class PolicyGradient:\n",
    "    # 强化学习的学习体，其原始版本位于\n",
    "    # https://github.com/princewen/tensorflow_practice/blob/master/RL/Policy_Gradient/RL_brain.py\n",
    "    # 因为仅在其实现上做了少量修改，故注释极少\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_actions,\n",
    "                 n_features,\n",
    "                 learning_rate = 0.0001,\n",
    "                 reward_decay = 0.99,\n",
    "                 output_graph = False):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "\n",
    "        self.ep_obs,self.ep_as,self.ep_rs = [],[],[]\n",
    "\n",
    "        self._build_net()\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            tf.summary.FileWriter(\"logs/\",self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # 该方法可以保存整个计算图以及当前的参数，主要是网络参数\n",
    "        # 可随时调用\n",
    "        saver = tf.train.Saver(max_to_keep=3)\n",
    "        saver.save(self.sess, 'my-model', global_step=0)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        # 该方法可以将整个计算图中的参数加载回来\n",
    "        # 但是这个写法只能维持一个计算图副本\n",
    "        # 随时调用都可以\n",
    "        saver = tf.train.Saver(max_to_keep=3)\n",
    "        saver.restore(self.sess, 'my-model'+ '-' + str(0))\n",
    "        \n",
    "    def _build_net(self):\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.tf_obs = tf.placeholder(tf.float32,[None,self.n_features],name='observation')\n",
    "            self.tf_acts = tf.placeholder(tf.int32,[None,],name='actions_num')\n",
    "            self.tf_vt = tf.placeholder(tf.float32,[None,],name='actions_value')\n",
    "\n",
    "        layer = tf.layers.dense(\n",
    "            inputs = self.tf_obs,\n",
    "            units = 20,\n",
    "            activation= tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.3),\n",
    "            bias_initializer= tf.constant_initializer(0.1),\n",
    "            name='fc1'\n",
    "        )\n",
    "        \n",
    "        layer2 = tf.layers.dense(\n",
    "            inputs = layer,\n",
    "            units = 20,\n",
    "            activation= tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.3),\n",
    "            bias_initializer= tf.constant_initializer(0.1),\n",
    "            name='fc2'\n",
    "        )\n",
    "        \n",
    "        layer3 = tf.layers.dense(\n",
    "            inputs = layer2,\n",
    "            units = 20,\n",
    "            activation= tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.3),\n",
    "            bias_initializer= tf.constant_initializer(0.1),\n",
    "            name='fc3'\n",
    "        )\n",
    "        \n",
    "        layer4 = tf.layers.dense(\n",
    "            inputs = layer3,\n",
    "            units = 20,\n",
    "            activation= tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.3),\n",
    "            bias_initializer= tf.constant_initializer(0.1),\n",
    "            name='fc4'\n",
    "        )\n",
    "\n",
    "        layer5 = tf.layers.dense(\n",
    "            inputs = layer4,\n",
    "            units = 20,\n",
    "            activation= tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.3),\n",
    "            bias_initializer= tf.constant_initializer(0.1),\n",
    "            name='fc5'\n",
    "        )\n",
    "        \n",
    "        all_act = tf.layers.dense(\n",
    "            inputs = layer5,\n",
    "            units = self.n_actions,\n",
    "            activation = None,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.3),\n",
    "            bias_initializer = tf.constant_initializer(0.1),\n",
    "            name='fc6'\n",
    "        )\n",
    "\n",
    "        self.all_act_prob = tf.nn.softmax(all_act,name='act_prob')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            #neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.all_act_prob,labels =self.tf_acts)\n",
    "\n",
    "            neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob) * tf.one_hot(indices=self.tf_acts,depth=self.n_actions),axis=1)\n",
    "            loss = tf.reduce_mean(neg_log_prob * self.tf_vt)\n",
    "\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "    def choose_action(self,observation):\n",
    "        prob_weights = self.sess.run(self.all_act_prob,feed_dict={self.tf_obs:observation[np.newaxis,:]})\n",
    "        action = np.random.choice(range(prob_weights.shape[1]),p=prob_weights.ravel())\n",
    "        return action\n",
    "\n",
    "\n",
    "    def store_transition(self,s,a,r):\n",
    "        self.ep_obs.append(s)\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        discounted_ep_rs_norm = self._discount_and_norm_rewards()\n",
    "\n",
    "        self.sess.run(self.train_op,feed_dict={\n",
    "            self.tf_obs:np.vstack(self.ep_obs),\n",
    "            self.tf_acts:np.array(self.ep_as),\n",
    "            self.tf_vt:discounted_ep_rs_norm,\n",
    "        })\n",
    "\n",
    "        self.ep_obs,self.ep_as,self.ep_rs = [],[],[]\n",
    "        return discounted_ep_rs_norm\n",
    "\n",
    "\n",
    "\n",
    "    def _discount_and_norm_rewards(self):\n",
    "        discounted_ep_rs = np.zeros_like(self.ep_rs)\n",
    "        running_add = 0\n",
    "        # reserved 返回的是列表的反序，这样就得到了贴现求和值。\n",
    "        for t in reversed(range(0,len(self.ep_rs))):\n",
    "            running_add = running_add * self.gamma + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "        \n",
    "        discounted_ep_rs = discounted_ep_rs - np.mean(discounted_ep_rs)\n",
    "        discounted_ep_rs = discounted_ep_rs / np.std(discounted_ep_rs)\n",
    "        return discounted_ep_rs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Policy Gradient, Reinforcement Learning.\n",
    "The cart pole example\n",
    "Using:\n",
    "Tensorflow: 1.0\n",
    "gym: 0.8.0\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = platform(steps=3)\n",
    "# 初始化平台对象\n",
    "# 但是实际上存在一个问题，该代码缺乏波浪力的随机生成过程，一直在用同一组波浪力...\n",
    "# 实在没办法可以先自己弄一堆波浪力文件，然后在env.reset()过程中进行随机选取\n",
    "\n",
    "n_actions = env.n_actions\n",
    "n_features = env.n_features\n",
    "\n",
    "RL = PolicyGradient(\n",
    "    n_actions=n_actions,\n",
    "    n_features = n_features,\n",
    "    learning_rate = 0.001,\n",
    "    reward_decay = 0.99\n",
    ")\n",
    "\n",
    "maxscore = -999999\n",
    "minmaxabsx = 2.4\n",
    "minstdx = 0.007\n",
    "# 模型表现效果的阈值\n",
    "\n",
    "for i_episode in range(3000):\n",
    "    # episode数\n",
    "    # 未设计模型暂存与加载功能，可以自己按照需求添加，相关方法已经提供了\n",
    "    \n",
    "    observation = env.reset()\n",
    "    # 重置平台对象\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        action = RL.choose_action(observation)\n",
    "        # 根据随机概率列选择动作\n",
    "        observation_,reward,done,info = env.step(action)\n",
    "        # 将动作送给平台对象，然后得到状态返回值\n",
    "        RL.store_transition(observation,action,reward)\n",
    "        # 调用学习体的存储方法\n",
    "\n",
    "        if done:\n",
    "            # 一轮学习完成后\n",
    "            ep_rs_sum = sum(RL.ep_rs)\n",
    "            # 将每步的得分累和\n",
    "            \n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                # 用一个公式计算模型当前得分\n",
    "                running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "            if running_reward > maxscore:\n",
    "                # 当得分高的时候，将模型的表现保存下来\n",
    "                maxscore = running_reward\n",
    "                print('maxscore updated:', running_reward)\n",
    "                env.save_status('maxscore.mat')\n",
    "            \n",
    "            \n",
    "            tmpval = env.maxabsx()\n",
    "            if tmpval < minmaxabsx:\n",
    "                # 当偏移最大值达到最小时暂存模型表现\n",
    "                minmaxabsx = tmpval\n",
    "                print('minabsx updated:', tmpval)\n",
    "                env.save_status('minabsx.mat')\n",
    "                \n",
    "            tmpval = env.stdx()\n",
    "            if tmpval < minstdx:\n",
    "                # 同理\n",
    "                minstdx = tmpval\n",
    "                print('minstdx updated:', tmpval)\n",
    "                env.save('minstdx.mat')\n",
    "                \n",
    "            print(\"episode:\", i_episode, \"  reward:\", running_reward)\n",
    "            # 打印该轮的数值表现，可扩充\n",
    "            \n",
    "            vt = RL.learn()\n",
    "            # 调用学习体的调参方法进行参数更新\n",
    "            \n",
    "            if i_episode % 100 == 0:\n",
    "                # 每100轮打印一下模型效果\n",
    "                print(\"showing...\")\n",
    "                env.showlast()\n",
    "                # 这个是平台历史状态的图\n",
    "                \n",
    "                plt.plot(vt)\n",
    "                plt.show()\n",
    "                # 这个是每步得分的图\n",
    "                \n",
    "            break\n",
    "            \n",
    "        observation = observation_\n",
    "        # observation 和 observation_的逻辑关系请自己理一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
